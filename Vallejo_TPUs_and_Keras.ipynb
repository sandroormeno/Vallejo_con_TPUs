{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vallejo TPUs and Keras",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sandroormeno/Vallejo_con_TPUs/blob/master/Vallejo_TPUs_and_Keras.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "bfJp0Q0M1npS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "metadata": {
        "id": "rxkNIKFM1vWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edfbxDDh2AEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict Shakespeare with Cloud TPUs and Keras"
      ]
    },
    {
      "metadata": {
        "id": "wYp6XVQU2POn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "xzpUtDMqmA-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a [Google Cloud TPU](https://cloud.google.com/tpu/). This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
        "\n",
        "We'll train the model on the combined works of William Shakespeare, then use it to compose a play in the style of *The Great Bard*:\n",
        "\n",
        "<blockquote>\n",
        "Loves that led me no dumbs lack her Berjoy's face with her to-day.  \n",
        "The spirits roar'd; which shames which within his powers  \n",
        "\tWhich tied up remedies lending with occasion,  \n",
        "A loud and Lancaster, stabb'd in me  \n",
        "\tUpon my sword for ever: 'Agripo'er, his days let me free.  \n",
        "\tStop it of that word, be so: at Lear,  \n",
        "\tWhen I did profess the hour-stranger for my life,  \n",
        "\tWhen I did sink to be cried how for aught;  \n",
        "\tSome beds which seeks chaste senses prove burning;  \n",
        "But he perforces seen in her eyes so fast;  \n",
        "And _    \n",
        "</blockquote>\n",
        "\n",
        "Note: To enable TPUs on Google Colab, select *Runtime > Change runtime type*, and set *Hardware acceleration* to TPU."
      ]
    },
    {
      "metadata": {
        "id": "KRQ6Fjra3Ruq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download data\n",
        "\n",
        "Download *The Complete Works of William Shakespeare* as a single text file from [Project Gutenberg](https://www.gutenberg.org/). We'll use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
      ]
    },
    {
      "metadata": {
        "id": "j8sIXh1DEDDd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "85fd3d43-23ee-4bc0-f651-ec262d719544"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!wget --show-progress --continue -O /content/vallejo.txt https://raw.githubusercontent.com/sandroormeno/Tensorflow-basic-examples/master/MNIST_data/vallejo3.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "/content/vallejo.tx 100%[===================>] 250.20K  --.-KB/s    in 0.02s   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AbL6cqCl7hnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the data generator"
      ]
    },
    {
      "metadata": {
        "id": "E3V4V-Jxmuv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "35c3edea-6376-4db4-f6e4-590a683abea1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "\n",
        "# This address identifies the TPU we'll use when configuring TensorFlow.\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "SHAKESPEARE_TXT = '/content/vallejo.txt'\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "def transform(txt, pad_to=None):\n",
        "  # drop any non-ascii characters\n",
        "  output = np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "  if pad_to is not None:\n",
        "    output = output[:pad_to]\n",
        "    output = np.concatenate([\n",
        "        np.zeros([pad_to - len(txt)], dtype=np.int32),\n",
        "        output,\n",
        "    ])\n",
        "  return output\n",
        "\n",
        "def training_generator(seq_len=100, batch_size=1024):\n",
        "  \"\"\"A generator yields (source, target) arrays for training.\"\"\"\n",
        "  with tf.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "    txt = f.read()\n",
        "\n",
        "  tf.logging.info('Input text [%d] %s', len(txt), txt[:50])\n",
        "  source = transform(txt)\n",
        "  while True:\n",
        "    offsets = np.random.randint(0, len(source) - seq_len, batch_size)\n",
        "\n",
        "    # Our model uses sparse crossentropy loss, but Keras requires labels\n",
        "    # to have the same rank as the input logits.  We add an empty final\n",
        "    # dimension to account for this.\n",
        "    yield (\n",
        "        np.stack([source[idx:idx + seq_len] for idx in offsets]),\n",
        "        np.expand_dims(\n",
        "            np.stack([source[idx + 1:idx + seq_len + 1] for idx in offsets]),\n",
        "            -1),\n",
        "    )\n",
        "\n",
        "six.next(training_generator(seq_len=10, batch_size=1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Input text [250593] ﻿\r\n",
            "\r\n",
            "Los Heraldos Negros\r\n",
            "(1918)\r\n",
            "\r\n",
            "\r\n",
            "LOS HERALDOS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[105, 100, 105, 111, 116,  97,  13,  10, 101, 115]], dtype=int32),\n",
              " array([[[100],\n",
              "         [105],\n",
              "         [111],\n",
              "         [116],\n",
              "         [ 97],\n",
              "         [ 13],\n",
              "         [ 10],\n",
              "         [101],\n",
              "         [115],\n",
              "         [101]]], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "Bbb05dNynDrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM—with two changes from the `tf.keras` standard LSTM definition:\n",
        "\n",
        "1. Define the input `shape` of our model which satisfies the [XLA compiler](https://www.tensorflow.org/performance/xla/)'s static shape requirement.\n",
        "2. Use `tf.train.Optimizer` instead of a standard Keras optimizer (Keras optimizer support is still experimental)."
      ]
    },
    {
      "metadata": {
        "id": "yLEM-fLJlEEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
        "  source = tf.keras.Input(\n",
        "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "  model = tf.keras.Model(inputs=[source], outputs=[predicted_char])\n",
        "  model.compile(\n",
        "      optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzBYDJI0_Tfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "The `tf.contrib.tpu.keras_to_tpu_model` function converts a `tf.keras` model to an equivalent TPU version. We then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`."
      ]
    },
    {
      "metadata": {
        "id": "ExQ922tfzSGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "d5c74e5a-d118-4fa2-d6c6-ad778da66900"
      },
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "training_model = lstm_model(seq_len=100, batch_size=128, stateful=False)\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    training_model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "\n",
        "tpu_model.fit_generator(\n",
        "    training_generator(seq_len=100, batch_size=1024),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10,\n",
        ")\n",
        "tpu_model.save_weights('/tmp/bard.h5', overwrite=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.99.189.98:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8869711600961521496)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 212061580876133003)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2690410430993031016)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7427259642736647781)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5974654814982514243)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 12175354480154809496)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4560449559285082905)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 6958338485208868126)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 12559433668596278516)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 15928701053319996949)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6266601347043838844)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 13607303928733081498)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Connecting to: b'grpc://10.99.189.98:8470'\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:Input text [250593] ﻿\n",
            "\n",
            "Los Heraldos Negros\n",
            "(1918)\n",
            "\n",
            "\n",
            "LOS HERALDOS\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train, [TensorSpec(shape=(128, 100), dtype=tf.int32, name='seed0'), TensorSpec(shape=(128, 100, 1), dtype=tf.float32, name='time_distributed_target_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for seed\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.819403648376465 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "100/100 [==============================] - 24s 244ms/step - loss: 4.4725 - sparse_categorical_accuracy: 0.1421\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 16s 160ms/step - loss: 3.1096 - sparse_categorical_accuracy: 0.1616\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 16s 161ms/step - loss: 1.9861 - sparse_categorical_accuracy: 0.3877\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 16s 161ms/step - loss: 1.2034 - sparse_categorical_accuracy: 0.6166\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 16s 160ms/step - loss: 0.5558 - sparse_categorical_accuracy: 0.8311\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 16s 160ms/step - loss: 0.3075 - sparse_categorical_accuracy: 0.9111\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 16s 162ms/step - loss: 0.2434 - sparse_categorical_accuracy: 0.9289\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 16s 160ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9333\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 16s 161ms/step - loss: 0.2051 - sparse_categorical_accuracy: 0.9391\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 16s 162ms/step - loss: 0.1980 - sparse_categorical_accuracy: 0.9412\n",
            "INFO:tensorflow:Copying TPU weights to the CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCBtcpZkykSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make predictions with the model\n",
        "\n",
        "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. We'll make five predictions from the initial seed."
      ]
    },
    {
      "metadata": {
        "id": "tU7M-EGGxR3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1258
        },
        "outputId": "d3db4285-394b-44d1-f58d-d84af189b69c"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Keras requires the batch size be specified ahead of time for stateful models.\n",
        "# We use a sequence length of 1, as we will be feeding in one character at a \n",
        "# time and predicting the next character.\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# We seed the model with our initial string, copied BATCH_SIZE times\n",
        "\n",
        "seed_txt = 'Looks it not like the king?  Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "  prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "  last_word = predictions[-1]\n",
        "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "  \n",
        "  # sample from our output distribution\n",
        "  next_idx = [\n",
        "      np.random.choice(256, p=next_probits[i])\n",
        "      for i in range(BATCH_SIZE)\n",
        "  ]\n",
        "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "  \n",
        "\n",
        "for i in range(BATCH_SIZE):\n",
        "  print('PREDICTION %d\\n\\n' % i)\n",
        "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "  generated = ''.join([chr(c) for c in p])\n",
        "  print(generated)\n",
        "  print()\n",
        "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREDICTION 0\n",
            "\n",
            "\n",
            " ¡BErro a morir\r\n",
            "a Pedro, a Rojas, al obrero, al hombre, aún al Medio de las rubios cantos,\r\n",
            "zu dolor es tan rifidad, y esto\r\n",
            "tiene muerto y tu costro y la sangre.\r\n",
            "El cuello a los ojos mucha pena\r\n",
            "y también en el otro, mucha pena\r\n",
            "y en los dos, cuan\n",
            "\n",
            "PREDICTION 1\n",
            "\n",
            "\n",
            " Buena gusta la vida enormemente\r\n",
            "pero, desde luego,\r\n",
            "con cuánto comcuendo crepúsculo de los muertos!\r\n",
            "¡Cuídate de tus muertos!\r\n",
            "¡Cuídate de la República!\r\n",
            "¡Cuídate del futuro!\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "XV\r\n",
            "\r\n",
            "ESPAÑA, APARTA DE MÍ ESTEJO\r\n",
            "Así pasa la vida, como raro e\n",
            "\n",
            "PREDICTION 2\n",
            "\n",
            "\n",
            " ¡Ah la pobre va en un entierro sollozando\r\n",
            "¿Cómo luego ingresar a la Academia?\r\n",
            "\r\n",
            "Alguien limpia un fusil en su cocina\r\n",
            "¿Con qué valor hablar del psicoanálisis?\r\n",
            "\r\n",
            "Otro ha entrado en mi pecho con un palo en la mano\r\n",
            "¿Hablar luego de Sócrates al minu\n",
            "\n",
            "PREDICTION 3\n",
            "\n",
            "\n",
            " Vuestra se sira para afuera,\r\n",
            "que suda hoy para adentro su secreción del alma.\r\n",
            "\r\n",
            "Padre polvo, biznieto del humo.\r\n",
            "\r\n",
            "Pero, naturalmente, vosotros sois hijos.\r\n",
            "\r\n",
            "      Inos el propio barro y propia nube sólida!\r\n",
            "Invierte o transcurrido alcanza\r\n",
            "para \n",
            "\n",
            "PREDICTION 4\n",
            "\n",
            "\n",
            " ¡Un solo sorda, mi abajo,\r\n",
            "marchar por los labios inferiores del destino!\r\n",
            "\r\n",
            "Es éste es el bien ardiendo,\r\n",
            "como granas atercidad.\r\n",
            "\r\n",
            "      Y llegue a brogado amado sea\r\n",
            "el que corre a ver su límite antes que arda.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "ProsirO RUENTO\r\n",
            "\r\n",
            "Al cabo, a\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}